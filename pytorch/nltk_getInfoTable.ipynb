{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import gutenberg as gb\n",
    "from nltk.util import pad_sequence\n",
    "from nltk.util import bigrams\n",
    "from nltk.util import ngrams\n",
    "from nltk.util import everygrams\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.lm.preprocessing import flatten\n",
    "from nltk import word_tokenize, sent_tokenize \n",
    "import pandas as pd\n",
    "#import lux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "古腾堡语料库 NLTK包含古腾堡项目（Project Gutenberg）电子文档的一小部分文本。 该项目大约有25000（现在是36000）本免费电子书。 我们通过平均句子长度和平均词种数（词语丰富度）这两个特征，来看不同作者的写作风格。\n",
    "\n",
    "NLTK的古腾堡语料库收集的都是不同作家的书"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用nltk的数据集\n",
    "#nltk.download('gutenberg')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('movie_reviews')#电影评论\n",
    "#nltk.download('webtext') #网络和聊天文本\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.corpus import webtext\n",
    "from nltk.corpus import movie_reviews\n",
    "#print(gutenberg.fileids())\n",
    "send_table = [] # 得到text的二维str数组\n",
    "text_string = '' # text纯string表示\n",
    "p = ''',.\";:'?()-{}=+/?()*&^%#;[]!`'''\n",
    "data_source = movie_reviews\n",
    "for text_id in data_source.fileids():\n",
    "    print(text_id)\n",
    "    unit = data_source.sents(text_id)\n",
    "    for s in unit:\n",
    "        sent = []\n",
    "        for word in s:\n",
    "            if len(word)==1 and word in p:\n",
    "                continue #在此增加if条件可以去除停用词等\n",
    "            word = word.lower()\n",
    "            sent.append(word)\n",
    "            text_string+=word+' '\n",
    "        send_table.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用torch上的数据集\n",
    "# import datasets\n",
    "import torch\n",
    "import string\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from torchtext.datasets import IMDB\n",
    "import torchtext2sendtable as t2s\n",
    "\n",
    "train_iter = IMDB(split='train')\n",
    "send_table = t2s.torchtext2nltkMLE(train_iter,30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "#send_table = []\n",
    "#sent1 = ['i', 'am', 'sam']\n",
    "#sent2 = ['sam' ,'i', 'am']\n",
    "#sent3 = ['i', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham']\n",
    "#send_table.append(sent1)\n",
    "#send_table.append(sent2)\n",
    "#send_table.append(sent3)\n",
    "text_string=''\n",
    "for s in send_table:\n",
    "    for word in s:\n",
    "        word = word.lower()\n",
    "        text_string+=word+' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 99526 samples and 5742728 outcomes>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i</td>\n",
       "      <td>84586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rented</td>\n",
       "      <td>330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>am</td>\n",
       "      <td>2653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>curious-yellow</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>from</td>\n",
       "      <td>19636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>my</td>\n",
       "      <td>11978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>video</td>\n",
       "      <td>1512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>store</td>\n",
       "      <td>485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>because</td>\n",
       "      <td>8783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>of</td>\n",
       "      <td>140952</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             word     cnt\n",
       "0               i   84586\n",
       "1          rented     330\n",
       "2              am    2653\n",
       "3  curious-yellow       3\n",
       "4            from   19636\n",
       "5              my   11978\n",
       "6           video    1512\n",
       "7           store     485\n",
       "8         because    8783\n",
       "9              of  140952"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对text_string进行词频统计\n",
    "word_list = []\n",
    "cnt_list = []\n",
    "text_tokenized = nltk.word_tokenize(text_string)\n",
    "#print(text_tokenized)   \n",
    "from nltk import FreqDist\n",
    "fdist = FreqDist(text_tokenized)\n",
    "print(fdist)\n",
    "for a,b in fdist.items():\n",
    "    word_list.append(str(a))\n",
    "    cnt_list.append(b)\n",
    "dic = {\n",
    "    'word':word_list,\n",
    "    'cnt':cnt_list\n",
    "}\n",
    "info_table = pd.DataFrame(dic)\n",
    "info_table.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_table.to_csv('./temp.csv')\n",
    "df = pd.read_csv('./temp.csv')\n",
    "from lux.vis.Vis import Vis\n",
    "intent= ['word','cnt']\n",
    "vis = Vis(intent,df)\n",
    "vis\n",
    "print(vis.to_matplotlib())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 118134 items>\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the tokenized text for 3-grams language modelling\n",
    "# send_table: list[list(str)]\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "n = 3\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, send_table)\n",
    "# 使用nltk 的ngram模型\n",
    "from nltk.lm import MLE\n",
    "model = MLE(n) # Lets train a 3-grams model, previously we set n=3\n",
    "model.fit(train_data, padded_sents)\n",
    "print(model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.031682074225803845"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score('am',['i'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-inf"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.logscore('cookie',['<s>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-gram\n",
    "word2sum = {}\n",
    "word2freq = {}\n",
    "for sent in send_table:\n",
    "    w1='<s>'\n",
    "    for word in sent:\n",
    "        freq = -model.logscore(word)\n",
    "        log_prob = -model.logscore(word,[w1])\n",
    "        #print(word+' :',end='')\n",
    "        #print(log_prob)\n",
    "        if word in word2sum:\n",
    "            word2sum[word] += log_prob\n",
    "            word2freq[word] += freq\n",
    "        else:\n",
    "            word2sum[word] = log_prob\n",
    "            word2freq[word] = freq\n",
    "        w1 = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-gram\n",
    "word2sum = {}\n",
    "word2freq = {}\n",
    "for sent in send_table:\n",
    "    w1=''\n",
    "    w2=''\n",
    "    for word in sent:\n",
    "        freq = -model.logscore(word)\n",
    "        if w2 == '':\n",
    "            log_prob = -model.logscore(word,['<s>'])\n",
    "        elif w1 == '':\n",
    "            log_prob = -model.logscore(word,[\"<s>\",w2])\n",
    "        else:\n",
    "            log_prob = -model.logscore(word,[w1,w2])\n",
    "        if word in word2sum:\n",
    "            word2sum[word] += log_prob\n",
    "            word2freq[word] += freq\n",
    "        else:\n",
    "            word2sum[word] = log_prob\n",
    "            word2freq[word] = freq\n",
    "        w1 = w2\n",
    "        w2 = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#增加词长、计算信息量\n",
    "word_count_table = pd.DataFrame()\n",
    "for n,word in enumerate(info_table['word']):\n",
    "    # Create a list of just the word we are interested in, we use regular expressions so that part of words do not count\n",
    "    # e.g. 'ear' would be counted in each appearance of the word 'year'\n",
    "    word_count = len(word)  \n",
    "    word_count_table = word_count_table.append(pd.DataFrame({'len':word_count}, index=[n]))\n",
    "info_table['len'] = word_count_table['len']\n",
    "info_table.head()\n",
    "sum_table = pd.DataFrame()\n",
    "freq_table = pd.DataFrame()\n",
    "for n,word in enumerate(info_table['word']):\n",
    "    if word not in word2sum:\n",
    "        sum_table = sum_table.append(pd.DataFrame({'sum':0}, index=[n]))\n",
    "        freq_table = freq_table.append(pd.DataFrame({'freq':0},index=[n]))\n",
    "        continue\n",
    "    sum_proc = word2sum[word]\n",
    "    freq = word2freq[word]\n",
    "    sum_table = sum_table.append(pd.DataFrame({'sum':sum_proc}, index=[n]))\n",
    "    freq_table = freq_table.append(pd.DataFrame({'freq':freq}, index=[n]))\n",
    "info_table['sum_neg_log_prob'] = sum_table['sum']\n",
    "info_table['sum_freq'] = freq_table['freq']\n",
    "info_table['averge_content'] = (info_table['sum_neg_log_prob']/info_table['cnt'])\n",
    "info_table['averge_freq'] = (info_table['sum_freq']/info_table['cnt'])\n",
    "info_table.head(30)\n",
    "info_table.to_csv('./info_table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_table.sort_values(by='cnt',ascending='',inplace=True)\n",
    "info_table.dtypes\n",
    "info_table = info_table.drop(info_table[info_table['word'].str.isdigit()==True].index)\n",
    "info_table.intent = ['len','averge_content']\n",
    "info_table.head(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_table = info_table[info_table['len']>4]\n",
    "info_table = info_table.iloc[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplot_section_cor as corshow\n",
    "import numpy as np\n",
    "corshow.show_cor_of(info_table,\"len\",\"averge_freq\",\"averge_content\",\"cnt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (vis.to_matplotlib())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spearman correlation coefficient\n",
    "#原始数据\n",
    "X1= info_table['len']\n",
    "Y1= info_table['averge_content']\n",
    "Z1= info_table['averge_freq']\n",
    "#s.corr()函数计算\n",
    "r=X1.corr(Y1,method='spearman')\n",
    "q=X1.corr(Z1,method='spearman')\n",
    "print(\"corelationship of len and averge_content: %f\",r)\n",
    "print(\"corelationship of len and freq: %f\",q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_table.plot.scatter(x=\"len\", y=\"averge_content\",alpha=0.2)\n",
    "plt.savefig('./3-gram_gutenberg_top_200.jpg', bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_table_lenle3 = info_table[info_table['len']>5]\n",
    "info_table_lenle3[info_table_lenle3['averge_content']<5].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_table.to_csv('./info_csv')\n",
    "import lux\n",
    "df = pd.read_csv('./info_csv')\n",
    "from lux.vis.Vis import Vis\n",
    "intent = ['averge_content',\"len\"]\n",
    "vis = Vis(intent,df)\n",
    "vis"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f87c41940a98a1a06bde016bc11ff418ea3d5a21d6fa78193519d8f174f35359"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('stopwords': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
